<!-- python 100本ノック 学習用のファイル -->

<!--

import os
from google.colab import drive
drive.mount('/content/drive')

///   knock31   ///  データ概要確認

import pandas as pd
uselog = pd.read_csv("/content/drive/My Drive/100knock/use_log4.csv")
uselog.isnull().sum()


customer = pd.read_csv("/content/drive/My Drive/100knock/customer_join4.csv")
customer.isnull().sum()

データ読み込みと欠損値の確認実施

/// /// /// /// ///


///   knock32   ///  クラスタリング、データのグループ化


customer_clustering = customer[["mean", "median", "max", "min", "membership_period"]]
customer_clustering.head()
顧客データグループ化のため、変数絞り込み



from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
customer_clustering_sc = sc.fit_transform(customer_clustering)

kmeans = KMeans(n_clusters = 4, random_state = 0)
clusters = kmeans.fit(customer_clustering_sc)
customer_clustering = customer_clustering.assign(cluster = clusters.labels_)

print(customer_clustering["cluster"].unique())
customer_clustering.head()

K-means法、標準化のためのscikit-learnライブラリ導入
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

標準化実行、新たに格納
sc = StandardScaler()
customer_clustering_sc = sc.fit_transform(customer_clustering)

K-meansのモデル構築、クラスタ数＝4とし、モデル定義
kmeans = KMeans(n_clusters = 4, random_state = 0)
clusters = kmeans.fit(customer_clustering_sc)
customer_clustering = customer_clustering.assign(cluster = clusters.labels_)

0-3の4グループが作成された


/// /// /// /// ///


///   knock33   ///  クラスタ結果

customer_clustering.columns = ["月内平均値", "月内中央値", "月内最大値", "月内最小値", "会員期間", "cluster"]
customer_clustering.groupby("cluster").count()

列名の変更
customer_clustering.columns = ["月内平均値", "月内中央値", "月内最大値", "月内最小値", "会員期間", "cluster"]

列をクラスタごとに集計
customer_clustering.groupby("cluster").count()

グループごとの平均
customer_clustering.groupby("cluster").mean()


/// /// /// /// ///


///   knock34   /// グラフ表示

from sklearn.decomposition import PCA
X = customer_clustering_sc
pca = PCA(n_components = 2)
pca.fit(X)
x_pca = pca.transform(X)
pca_df = pd.DataFrame(x_pca)
pca_df["cluster"] = customer_clustering["cluster"]

主成分分析
ライブラリ導入
from sklearn.decomposition import PCA

モデル定義
pca = PCA(n_components = 2)

主成分分析実施
pca.fit(X)
x_pca = pca.transform(X)

pca_df：二次元に削減したデータを格納


import matplotlib.pyplot as plt
%matplotlib inline
for i in customer_clustering["cluster"].unique():
  tmp = pca_df.loc[pca_df["cluster"] == i]
  plt.scatter(tmp[0], tmp[1])

ライブラリ導入
import matplotlib.pyplot as plt

グラフ表示
%matplotlib inline

散布図にプロット
for i in customer_clustering["cluster"].unique():
  tmp = pca_df.loc[pca_df["cluster"] == i]
  plt.scatter(tmp[0], tmp[1])


/// /// /// /// ///



///   knock35   ///  退会者の傾向把握

customer_clustering = pd.concat([customer_clustering, customer], axis = 1)
customer_clustering.groupby(["cluster", "is_deleted"], as_index = False).count()[["cluster", "is_deleted", "customer_id"]]

データ結合
customer_clustering = pd.concat([customer_clustering, customer], axis = 1)

各クラスタでの集計
customer_clustering.groupby(["cluster", "is_deleted"], as_index = False).count()[["cluster", "is_deleted", "customer_id"]]

定期利用の確認
customer_clustering.groupby(["cluster", "routine_flg"], as_index = False).count()[["cluster", "routine_flg", "customer_id"]]


/// /// /// /// ///



///   knock36   ///  翌月の利用回数予測のための準備

uselog["usedate"] = pd.to_datetime(uselog["usedate"])
uselog["年月"] = uselog["usedate"].dt.strftime("%Y%m")
uselog_months = uselog.groupby(["年月", "customer_id"], as_index = False).count()
uselog_months.rename(columns = {"log_id":"count"}, inplace = True)
del uselog_months["usedate"]
uselog_months.head()


year_months = list(uselog_months["年月"].unique())
predict_data = pd.DataFrame()
for i in range(6, len(year_months)):
  tmp = uselog_months.loc[uselog_months["年月"] == year_months[i]].copy()
  tmp.rename(columns = {"count":"count_pred"}, inplace = True)
  for j in range(1, 7):
    tmp_before = uselog_months.loc[uselog_months["年月"] == year_months[i-j]].copy()
    del tmp_before["年月"]
    tmp_before.rename(columns = {"count":"count_{}".format(j-i)}, inplace = True)
    tmp = pd.merge(tmp, tmp_before, on = "customer_id", how = "left")
  predict_data = pd.concat([predict_data, tmp], ignore_index = True)
predict_data.head()


predict_data = predict_data.dropna()
predict_data = predict_data.reset_index(drop = True)
predict_data.head()


/// /// /// /// ///





-->