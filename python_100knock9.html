<!-- python 100本ノック 学習用のファイル -->

<!--

import os
from google.colab import drive
drive.mount('/content/drive')

///   knock81   ///  画像読み込み

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread('/content/drive/My Drive/100knock/img/img01.jpg')
height, width = img.shape[:2]
print("画像幅：" + str(width))
print("画像高さ：" + str(height))
cv2_imshow(img)

/// /// /// /// ///


///   knock82   ///  動画読み込み

import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML

# 情報取得
cap = cv2.VideoCapture('/content/drive/My Drive/100knock/mov/mov01.avi')
width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
fps = cap.get(cv2.CAP_PROP_FPS)
print("画像幅：" + str(width))
print("画像高さ：" + str(height))
print("総フレーム数：" + str(count))
print("FPS：" + str(fps)) 

# 映像のフレーム画像化
num = 0
num_frame = 100
list_frame = []
while cap.isOpened():

    # 処理（フレームごとに切り出し）
    ret, frame = cap.read()

    # 出力（フレーム画像を書き出し）
    if ret:
      frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
      list_frame.append(frame)
      if cv2.waitKey(1) & 0xFF == ord('q'):
        break
      if num > num_frame:
        break
    num = num + 1
print("処理を完了しました")
cap.release()

# フレーム画像をアニメーションに変換
plt.figure()
patch = plt.imshow(list_frame[0])
plt.axis('off')
def animate(i):
    patch.set_data(list_frame[i])
anim = FuncAnimation(plt.gcf(), animate, frames=len(list_frame), interval=1000/30.0)
plt.close()

# アニメーション表示
HTML(anim.to_jshtml())  

/// /// /// /// ///


///   knock83   ///  動画の分割、保存

cap = cv2.VideoCapture('/content/drive/My Drive/100knock/mov/mov01.avi')
num = 0
count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
while(cap.isOpened()):
  ret, frame = cap.read()
  if ret:
    filepath = '/content/drive/My Drive/100knock/snapshot/snapshot_' + str(num) + '.jpg'
    cv2.imwrite(filepath, frame)
  num = num + 1
  if num >= count:
    break
cap.release()
cv2.destroyAllWindows()

/// /// /// /// ///


///   knock84   ///  画像から人物を検出

# 準備
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0}

# 検出
img = cv2.imread('/content/drive/My Drive/100knock/img/img01.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
human, r = hog.detectMultiScale(gray, **hogParams)

if len(human) > 0:
  for (x, y, w, h) in human:
    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 255), 3)
cv2_imshow(img)
cv2.imwrite('/content/drive/My Drive/100knock/img/temp01.jpg', img)

/// /// /// /// ///


///   knock75   ///  パラメータ全体像の相図描画

/// /// /// /// ///


///   knock76   ///  データの読み込み

/// /// /// /// ///



///   knock77   ///  ヒストグラム表示

/// /// /// /// ///



///   knock78   ///  実データからの推定

/// /// /// /// ///


///   knock79   ///  実データとシミュレーションの比較

/// /// /// /// ///


///   knock80   ///  シミュレーション実施

/// /// /// /// ///


-->