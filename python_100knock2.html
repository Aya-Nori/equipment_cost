<!-- python 100本ノック 学習用のファイル -->

<!--

///   knock11   ///  データ読み込み、データ確認

import os
from google.colab import drive
drive.mount('/content/drive')

データの確認 データの揺れの確認
フォーマット、表記の一致を確認する

import pandas as pd
uriage_data = pd.read_csv('/content/drive/My Drive/100knock/uriage.csv')
uriage_data.head()

kokyaku_data = pd.read_excel("/content/drive/My Drive/100knock/kokyaku_daicho.xlsx")
kokyaku_data.head()


/// /// /// /// ///



///   knock12   ///   データの揺れ確認


データの確認 データの揺れの確認
フォーマット、表記の一致を確認する

uriage_data["item_name"].head()

uriage_data["item_price"].head()

/// /// /// /// ///



///   knock13   ///   仮集計

uriage_data["purchase_date"] = pd.to_datetime(uriage_data["purchase_date"])
uriage_data["purchase_month"] = uriage_data["purchase_date"].dt.strftime("%Y%m")
res = uriage_data.pivot_table(index = "purchase_month", columns = "item_name", aggfunc = "size", fill_value = 0)
res

売上履歴から商品ごとの月別売り上げ合計を集計
日付型の変換
uriage_data["purchase_date"] = pd.to_datetime(uriage_data["purchase_date"])
uriage_data["purchase_month"] = uriage_data["purchase_date"].dt.strftime("%Y%m")

縦軸：購入年月、横軸：商品件数
res = uriage_data.pivot_table(index = "purchase_month", columns = "item_name", aggfunc = "size", fill_value = 0)

集計結果表示
res

7 rows × 99 columns　という表示から、26columsよりも多く、色んな表記があって統一できていないことがわかる

/// /// /// /// ///



///   knock14   ///   データ揺れ補正
print(len(pd.unique(uriage_data["item_name"])))

ユニーク数の確認
print(len(pd.unique(uriage_data["item_name"])))


uriage_data["item_name"] = uriage_data["item_name"].str.upper()
uriage_data["item_name"] = uriage_data["item_name"].str.replace("　", "")
uriage_data["item_name"] = uriage_data["item_name"].str.replace(" ", "")
uriage_data.sort_values(by = ["item_name"], ascending = True)

小文字→大文字
uriage_data["item_name"] = uriage_data["item_name"].str.upper()

半角全角スペース除去
uriage_data["item_name"] = uriage_data["item_name"].str.replace("　", "")
uriage_data["item_name"] = uriage_data["item_name"].str.replace(" ", "")

データソート
uriage_data.sort_values(by = ["item_name"], ascending = True)


print(pd.unique(uriage_data["item_name"]))
print(len(pd.unique(uriage_data["item_name"])))


一覧表示により、結果を検証する
print(pd.unique(uriage_data["item_name"]))

ユニーク数の確認
print(len(pd.unique(uriage_data["item_name"])))


/// /// /// /// ///



///   knock15   ///   欠損値

uriage_data.isnull().any(axis = 0)
isnull()で欠損値の有無を把握


flg_is_null = uriage_data["item_price"].isnull()
for trg in list(uriage_data.loc[flg_is_null, "item_name"].unique()):
  price = uriage_data.loc[(~flg_is_null) & (uriage_data["item_name"] == trg), "item_price"].max()
  uriage_data.loc[(flg_is_null) & (uriage_data["item_name"] == trg), "item_price"] = price
uriage_data.head()

欠損部の確認
flg_is_null = uriage_data["item_price"].isnull()

ループ処理
list() 変数をリスト形式に変換
uriage_data.loc[flg_is_null, "item_name"] .loc関数条件に合致するデータを抽出
flg_is_null が欠損している行の item_name 列を抽出
unique() 抽出した商品名の重複をなくす
: 以降にループ処理
欠損値がある商品名 を用いて、同じ商品で金額が正しく記載されている行を .loc で探し、金額を取得
~flg_is_null 否定演算子 flg_is_null == False と同義
for trg in list(uriage_data.loc[flg_is_null, "item_name"].unique()):
  price = uriage_data.loc[(~flg_is_null) & (uriage_data["item_name"] == trg), "item_price"].max()

取得した金額でデータを補完  
売上履歴の item_price 列に対して .loc を行い、欠損を起こしている対象データを抽出し、 price を代入
  uriage_data.loc[(flg_is_null) & (uriage_data["item_name"] == trg), "item_price"] = price
uriage_data.head()

欠損値がないことを確認
uriage_data.isnull().any(axis = 0)

各商品の金額が正しく補完されたか確認
for trg in list(uriage_data["item_name"].sort_values().unique()):
  print(trg + "の最大値：" + str(uriage_data.loc[uriage_data["item_name"] == 
  trg] ["item_price"].max()) + "の最小値：" + str(uriage_data.loc[uriage_data
  ["item_name"] == trg]["item_price"].min(skipna = False)))

全ての商品についてループ処理
for trg in list(uriage_data["item_name"].sort_values().unique()):

ループ処理内で、各商品の金額の最大値と最小値を出力
最大値と最小値が同じであれば、金額はどれも同じ
skipna = False NaNデータ を無視するかの設定、NaNがある場合、最小値は NaN と表示される

/// /// /// /// ///


///   knock16   ///   顧客名のゆれ補正、統一

データ確認
kokyaku_data["顧客名"].head()
uriage_data["customer_name"].head()

顧客名からスペース除去
kokyaku_data["顧客名"] = kokyaku_data["顧客名"].str.replace("　","")
kokyaku_data["顧客名"] = kokyaku_data["顧客名"].str.replace(" ","")
kokyaku_data["顧客名"].head()


/// /// /// /// ///


///   knock17   ///   日付のゆれ補正、統一

数値表示数の抽出
flg_is_serial = kokyaku_data["登録日"].astype("str").str.isdigit()
flg_is_serial.sum()

fromSerial = pd.to_timedelta(kokyaku_data.loc[flg_is_serial, "登録日"].astype("float") - 2, unit = "D") + pd.to_datetime('1900/1/1')
fromSerial


数値を日付に変換する
pd.to_timedelta() 関数
astype("float") - 2 Excelとpythonの日付シリアル差の修正のための-2

日付書式の統一 スラッシュ区切りをハイフン区切りにする
fromString = pd.to_datetime(kokyaku_data.loc[~flg_is_serial, "登録日"])
fromString

数値→日付データとスラッシュ→ハイフンデータの統合
concat 関数を使用
kokyaku_data["登録日"] = pd.concat([fromSerial, fromString])
kokyaku_data

登録日から登録月を算出し、集計
groupby()の後、count()でデータ数集計
kokyaku_data["登録年月"] = kokyaku_data["登録日"].dt.strftime("%Y%m")
rslt = kokyaku_data.groupby("登録年月").count()["顧客名"]
print(rslt)
print(len(kokyaku_data))

検証として総データ数の出力
print(len(kokyaku_data))

数値データがないことを最初と同じコードで確認する
flg_is_serial = kokyaku_data["登録日"].astype("str").str.isdigit()
flg_is_serial.sum()

/// /// /// /// ///


///   knock18   ///   データ結合 顧客名をキーにデータ結合

join_data = pd.merge(uriage_data, kokyaku_data, left_on = "customer_name", right_on = "顧客名", how = "left")
join_data = join_data.drop("customer_name", axis = 1)
join_data


uriage_data データフレームの "customer_name" 列と kokyaku_data データフレームの "顧客名" 列をキーとして、
左外部結合（left join）を行っています。つまり、uriage_data のすべての行を保持しながら、kokyaku_data の該当する行がある場合には、その行も結合されます。

左外部結合を指定しているので、uriage_data の行が基準となり、kokyaku_data の該当する行があれば結合されます。
もし該当する行がない場合、結合後のデータフレームには NaN が含まれることになります。
join_data = pd.merge(uriage_data, kokyaku_data, left_on = "customer_name", right_on = "顧客名", how = "left")

結合方法
how = "left"

/// /// /// /// ///


///   knock19   /// ダンプ（ファイル出力）

join_dataから必要な列を抽出し、任意に並び替えた
dump_data = join_data[["purchase_date", "purchase_month", "item_name", "item_price", "顧客名", "かな", "地域", "メールアドレス", "登録日"]]
dump_data

join_dataから必要な列を抽出し、任意に並び替えた

csv形式でのファイル出力
dump_data.to_csv("dump_data.csc", index = False)

/// /// /// /// ///









-->